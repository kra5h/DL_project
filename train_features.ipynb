{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 740M (CNMeM is disabled, CuDNN 5105)\n",
      "/home/nvvaulin/.local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your CuDNN version is more recent then Theano. If you see problems, try updating Theano or downgrading CuDNN to version 4.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from StringIO import StringIO\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class Cell:\n",
    "    def __init__(self,v,p,fp,e):\n",
    "        self.e = e\n",
    "        self.p = p.copy()\n",
    "        self.fp = fp.copy()\n",
    "        self.v = v.copy()\n",
    "        \n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "all_data = load_obj('all_data')\n",
    "all_data = [i for i in all_data if 6 < len(i.p)<= 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from broadcast import *\n",
    "from lasagne.nonlinearities import *\n",
    "\n",
    "import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_theta_R(v,p):#v:bs,v_num,axis\n",
    "    theta = np.empty((len(v),2,p.shape[1],p.shape[1],p.shape[1]),dtype=np.float32)\n",
    "    R = np.empty((len(v),1,p.shape[1],p.shape[1]),dtype=np.float32)\n",
    "    v_norm = v/np.sqrt((v**2).sum(-1))[...,None]\n",
    "    pos = (v_norm[:,None,:,:]*p[:,:,None,:]).sum(-1)\n",
    "    R[:,0,:,:] = np.sqrt(((pos[:,:,None,:] - pos[:,None,:,:])**2).sum(-1))\n",
    "    dist = pos[:,:,None,:]-pos[:,None,:,:]\n",
    "    for i in range(dist.shape[1]):\n",
    "        dist[:,i,i,:] = 1.\n",
    "    dist /= np.sqrt((dist**2).sum(-1))[...,None]\n",
    "    \n",
    "    for i in range(dist.shape[1]):\n",
    "        dist[:,i,i,:] = 0.\n",
    "    theta[:,0,:,:,:] = (dist[:,:,None,:]*dist[:,None,:,:]).sum(-1)\n",
    "    \n",
    "    theta[:,1,:,:,:] = ((dist[:,:,:,None,1]*dist[:,:,None,:,2])-(dist[:,:,:,None,2]*dist[:,:,None,:,1]))**2+\\\n",
    "                       ((dist[:,:,:,None,0]*dist[:,:,None,:,2])-(dist[:,:,:,None,2]*dist[:,:,None,:,0]))**2+\\\n",
    "                       ((dist[:,:,:,None,0]*dist[:,:,None,:,1])-(dist[:,:,:,None,1]*dist[:,:,None,:,0]))**2\n",
    "    return theta,R\n",
    "    \n",
    "\n",
    "def train_data_generator(train_data,_batch_size,batches_per_epoch):\n",
    "    natoms = []\n",
    "    unique,natoms,counts = np.unique(np.array([len(i.p) for i in train_data]),return_inverse=True,return_counts=True)\n",
    "    unique_inx = np.arange(len(unique),dtype=np.int)\n",
    "    natom_inx = []  \n",
    "    tmp = np.arange(len(natoms),dtype=np.int)\n",
    "    for i in unique_inx:\n",
    "        natom_inx.append(tmp[natoms==i])\n",
    "    w = counts.astype(np.float32)\n",
    "    w /= w.sum()\n",
    "    for batch_num in xrange(batches_per_epoch):\n",
    "        count = np.random.choice(unique_inx,p=w)\n",
    "        \n",
    "        if(unique[count] <= 12):\n",
    "            batch_size = _batch_size\n",
    "        else:\n",
    "            batch_size = 1\n",
    "            \n",
    "        inx = np.random.choice(natom_inx[count],batch_size)        \n",
    "        batch_y = np.empty((batch_size,1),dtype=np.float32)\n",
    "        batch_p = np.empty((batch_size,unique[count],3),dtype=np.float32)\n",
    "        batch_v = np.empty((batch_size,3,3),dtype=np.float32)\n",
    "        for j,i in enumerate(inx):\n",
    "            batch_p[j] = train_data[i].p\n",
    "            batch_v[j] = train_data[i].v\n",
    "            batch_y[j,0] = train_data[i].e\n",
    "        batch_y = (batch_y - energy_mean)/energy_var\n",
    "        theta,R = make_theta_R(batch_v,batch_p)\n",
    "        yield  theta,R,batch_y.reshape(-1,1)\n",
    "        \n",
    "def val_data_generator(data,_batch_size):\n",
    "    lens = np.array([len(i.p) for i in data])\n",
    "    inxs = np.argsort(lens)\n",
    "    lens = lens[inxs]\n",
    "    lo = 0\n",
    "    while(lo < len(inxs)):\n",
    "        natoms = lens[lo]\n",
    "        if(natoms <= 12):\n",
    "            batch_size = _batch_size\n",
    "        else:\n",
    "            batch_size = 1\n",
    "        hi = min(lo+batch_size,np.searchsorted(lens[lo:],natoms,'right')+lo)    \n",
    "        batch_y = np.empty((hi-lo,1),dtype=np.float32)\n",
    "        batch_p = np.empty((hi-lo,natoms,3),dtype=np.float32)\n",
    "        batch_v = np.empty((hi-lo,3,3),dtype=np.float32)\n",
    "        for j,i in enumerate(inxs[lo:hi]):\n",
    "            batch_p[j] = data[i].p\n",
    "            batch_v[j] = data[i].v\n",
    "            batch_y[j,0] = data[i].e\n",
    "        lo = hi\n",
    "        batch_y = (batch_y - energy_mean)/energy_var\n",
    "        theta,R = make_theta_R(batch_v,batch_p)\n",
    "        yield  theta,R,batch_y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(data,val_ratio,seed = 0):\n",
    "    inx = np.arange(len(data),dtype=int)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(inx)\n",
    "    sl = int(val_ratio*len(data))\n",
    "    train_X = []\n",
    "    val_X = []\n",
    "    for i in range(sl):\n",
    "        val_X.append(data[inx[i]])\n",
    "    for i in range(sl,len(data)):\n",
    "        train_X.append(data[inx[i]])\n",
    "    return train_X,val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_features = all_data\n",
    "energy = np.array([i.e for i in  data_features])\n",
    "energy_mean = energy.mean()\n",
    "energy_var = np.sqrt(((energy-energy_mean)**2).mean())\n",
    "train_data,val_data = train_test_split(data_features,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 12.368 -2.67013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_generator = lambda : train_data_generator(train_data,5,200)\n",
    "val_generator = lambda : val_data_generator(val_data,5)\n",
    "\n",
    "for i in train_generator():\n",
    "    print i[0].max(),i[1].max(),i[2].max()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from lasagne.layers import Layer\n",
    "import numpy as np\n",
    "\n",
    "class AxisSumLayer(Layer):\n",
    "    def __init__(self, incoming, axis, **kwargs):\n",
    "        self.incoming_ndim = len(incoming.output_shape)\n",
    "        self.axis = axis\n",
    "        assert self.axis < self.incoming_ndim\n",
    "        super(AxisSumLayer, self).__init__(incoming, **kwargs)\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        self.symbolic_input_shape = input.shape\n",
    "        res = T.sum(input,self.axis)\n",
    "        return res\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        output_shape = [input_shape[i] for i in range(len(input_shape)) if i != self.axis]\n",
    "        return tuple(output_shape)\n",
    "\n",
    "def shuffle(axis,_shape):\n",
    "    return [_shape[axis]]+_shape[:axis]+_shape[axis+1:]\n",
    "\n",
    "\n",
    "def inv_shuffle(axis,_shape):\n",
    "    shape = list(_shape)    \n",
    "    return shape[1:axis+1]+[shape[0]]+shape[axis+1:]\n",
    "\n",
    "class AxisMulLayer(lasagne.layers.MergeLayer):\n",
    "    def __init__(self, incoming1, incoming2,axis, **kwargs):\n",
    "        self.axis = axis\n",
    "        super(AxisMulLayer, self).__init__([incoming1, incoming2], **kwargs)\n",
    "        \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        # (rows of first input x columns of second input)\n",
    "        return self.input_shapes[0]\n",
    "    \n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        tmp = inputs[0].dimshuffle(shuffle(self.axis,range(len(list(inputs[0].shape)))))\n",
    "        tmp = tmp*inputs[1]\n",
    "        tmp = tmp.dimshuffle(inv_shuffle(self.axis,range(len(list(inputs[0].shape)))))\n",
    "        return tmp\n",
    "    \n",
    "\n",
    "class AxisAddLayer(lasagne.layers.MergeLayer):\n",
    "    def __init__(self, incoming1, incoming2,axis, **kwargs):\n",
    "        super(AxisMulLayer, self).__init__([incoming1, incoming2], **kwargs)\n",
    "        \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        # (rows of first input x columns of second input)\n",
    "        return self.input_shapes[0]\n",
    "    \n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        tmp = inputs[0].dimshuffle(shuffle(self.axis,list(inputs[0].shape)))\n",
    "        tmp = tmp+inputs[1]\n",
    "        tmp = tmp.simshuffle(inv_shuffle(self.axis,list(inputs[0].shape)))\n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'tensor5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-35a329de8c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'theta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'R'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0menergies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'energy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'tensor5'"
     ]
    }
   ],
   "source": [
    "def build(theta,R,theta_shape,R_shape):#theta: bs,nf,n,n,n R:bs,nf,n,n\n",
    "    def make_dense(l,n,nonl = rectify):\n",
    "        res = BatchNormLayer(l)\n",
    "        res = DenseLayer(res,n,nonlinearity=nonl)\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    \n",
    "    theta_l = InputLayer(theta_shape,input_var = theta)\n",
    "    bcast = BroadcastLayer(theta_l,[0,2,3,4])\n",
    "    th_dense = bcast\n",
    "    th_tanh   = UnbroadcastLayer(make_dense(th_dense,8,tanh),bcast)\n",
    "    th_rect   = UnbroadcastLayer(make_dense(th_dense,8,rectify),bcast)\n",
    "    \n",
    "    R_l = InputLayer(R_shape,R)\n",
    "    bcast = BroadcastLayer(R_l,[0,2,3])\n",
    "    R_dense = bcast\n",
    "    R_rect_m = UnbroadcastLayer(make_dense(R_dense,8,rectify),bcast)   \n",
    "    R_rect_a = UnbroadcastLayer(make_dense(R_dense,16,rectify),bcast)\n",
    "    \n",
    "    thR1 = AxisMulLayer(th_tanh,R_rect_m,2)\n",
    "    thR2 = AxisMulLayer(th_tanh,R_rect_m,3)\n",
    "    thR3 = AxisMulLayer(th_tanh,R_rect_m,4)\n",
    "    thR_sum = ElemwiseSumLayer([thR1,thR2,thR3,th_rect])\n",
    "    bcast = BroadcastLayer(thR_sum,[0,2,3,4])\n",
    "    thR_dense = make_dense(bcast,16)\n",
    "    thR_sum = UnbroadcastLayer(thR_dense,bcast)\n",
    "    \n",
    "    gl_pool = AxisSumLayer(thR_sum,4)\n",
    "    R1 = ElemwiseSumLayer([R_rect_a,gl_pool])\n",
    "    \n",
    "    bcast = BroadcastLayer(R1,[0,2,3])\n",
    "    R1 = make_dense(bcast,16)\n",
    "    R1 = UnbroadcastLayer(R1,bcast)\n",
    "    \n",
    "    gl_pool = AxisSumLayer(R1,3)\n",
    "    bcast = BroadcastLayer(gl_pool,[0,2])\n",
    "    gl_pool = make_dense(bcast,64)\n",
    "    gl_pool = UnbroadcastLayer(gl_pool,bcast)\n",
    "    \n",
    "    gl_pool = AxisSumLayer(gl_pool,2)\n",
    "    res = make_dense(gl_pool,1,nonl=identity)\n",
    "    return res\n",
    "\n",
    "theta = T.tensor5('theta')\n",
    "R = T.tensor4('R')\n",
    "energies = T.matrix('energy')\n",
    "net = build(theta,R,(None,2,None,None,None),(None,1,None,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = get_output(net)\n",
    "params = get_all_params(net, trainable=True)\n",
    "loss = lasagne.objectives.squared_error(pred,energies).mean()\n",
    "G_lr = theano.shared(np.array(0.001, dtype=theano.config.floatX))\n",
    "updates = lasagne.updates.adam(loss,params,G_lr)\n",
    "print ('start')\n",
    "train_fn = theano.function([theta,R,energies],[loss], allow_input_downcast=True, updates=updates)\n",
    "print ('train_fn compiled')\n",
    "test_fn = theano.function([theta,R,energies],[loss], allow_input_downcast=True)\n",
    "print ('test_fn compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_weights(network, name ):\n",
    "    np.savez(name+\".npz\", **{\"param%d\" % i: param for i, param in enumerate(get_all_param_values(network))})\n",
    "             \n",
    "def load_weights(network,name ):\n",
    "    f = np.load(name+\".npz\")\n",
    "    params = [f[\"param%d\" % i] for i in range(len(f.files))]\n",
    "    f.close()\n",
    "    set_all_param_values(network,params)\n",
    "\n",
    "def train(num_epoch,train_data_genertor,val_data_genertor,\n",
    "          train_fn=train_fn,\n",
    "          test_fn=test_fn,\n",
    "          net=net,\n",
    "          model_prefix='model',\n",
    "          G_lr = G_lr,\n",
    "          lr_sh = [100,150]):\n",
    "    \n",
    "    log = open('train.log','a')\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        train_num_batch = 0.\n",
    "        train_loss = 0.\n",
    "        for i,batch in enumerate(train_data_genertor()):\n",
    "            train_loss += train_fn(batch[0],batch[1],batch[2])[0]\n",
    "            train_num_batch += 1.\n",
    "            if(i % 10 == 0):\n",
    "                print '\\r %i %f'%(i,train_loss/train_num_batch),\n",
    "                log.write('%i %f\\n'%(i,train_loss/train_num_batch))\n",
    "        val_loss = 0.\n",
    "        val_num_batch = 0.\n",
    "        for i,batch in enumerate(val_data_genertor()):\n",
    "            val_loss += test_fn(batch[0],batch[1],batch[2])[0]\n",
    "            val_num_batch += 1.\n",
    "        save_weights(net,model_prefix+'repoch%itr%.3fval%.3f'%(epoch,train_loss/train_num_batch,val_loss/val_num_batch))\n",
    "        print ('\\repoch %i train_loss=%f val_loss=%f'%(epoch,train_loss/train_num_batch,val_loss/val_num_batch))\n",
    "        log.write('epoch %i train_loss=%f val_loss=%f\\n'%(epoch,train_loss/train_num_batch,val_loss/val_num_batch))\n",
    "        \n",
    "        if(epoch in lr_sh):\n",
    "            G_lr.set_value(G_lr.get_value()*np.float(0.01))\n",
    "        \n",
    "    log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_generator = lambda : val_data_generator(val_data,30)\n",
    "G_lr.set_value(np.float(0.01))\n",
    "train(250,train_generator,val_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
