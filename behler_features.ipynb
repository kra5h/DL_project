{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from StringIO import StringIO\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class Cell:\n",
    "    def __init__(self,v,p,fp,e):\n",
    "        self.e = e\n",
    "        self.p = p.copy()\n",
    "        self.fp = fp.copy()\n",
    "        self.v = v.copy()\n",
    "\n",
    "        \n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "data_dir = 'data_behler'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_fetures(data_dir):\n",
    "    data_X = []\n",
    "    data_y = []\n",
    "    data_names = []\n",
    "    for num,i in enumerate(os.listdir(data_dir)):\n",
    "        name = data_dir+'/'+i\n",
    "        with open(name, 'rb') as f:\n",
    "            arr = pickle.load(f)\n",
    "        X = np.empty((len(arr.keys())-1,arr[0].shape[0]),dtype=np.float32)\n",
    "        for j in range(len(arr.keys())-1):\n",
    "            X[j] = arr[j]\n",
    "        data_X.append(X)\n",
    "        data_y.append(arr[-1])\n",
    "        data_names.append(int(i[:-len('.pkl')]))\n",
    "        \n",
    "        if(num % 10 == 0):\n",
    "            print '\\r %i'%num,\n",
    "    return data_X,np.array(data_y),np.array(data_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8180                                                                                                                                                                                                          \n"
     ]
    }
   ],
   "source": [
    "bechler_f = read_fetures('data_behler')\n",
    "save_obj(bechler_f,'bechler_f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8180                                                                                                                                                                                                                                                                                                               \n"
     ]
    }
   ],
   "source": [
    "#cheb_f = read_fetures('data_cheb')\n",
    "#save_obj(cheb_f,'cheb_f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 740M (CNMeM is disabled, CuDNN 5105)\n",
      "/home/nvvaulin/.local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your CuDNN version is more recent then Theano. If you see problems, try updating Theano or downgrading CuDNN to version 4.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import *\n",
    "from broadcast import *\n",
    "from lasagne.nonlinearities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_data_generator(train_data,batch_size,batches_per_epoch):\n",
    "    data_X = train_data[0]\n",
    "    data_y = train_data[1]\n",
    "    natoms = []\n",
    "    unique,natoms,counts = np.unique(np.array([len(i) for i in data_X]),return_inverse=True,return_counts=True)\n",
    "    unique_inx = np.arange(len(unique),dtype=np.int)\n",
    "    natom_inx = []  \n",
    "    tmp = np.arange(len(natoms),dtype=np.int)\n",
    "    for i in unique_inx:\n",
    "        natom_inx.append(tmp[natoms==i])\n",
    "    w = counts.astype(np.float32)\n",
    "    w /= w.sum()\n",
    "    for batch_num in xrange(batches_per_epoch):\n",
    "        count = np.random.choice(unique_inx,p=w)\n",
    "        inx = np.random.choice(natom_inx[count],batch_size)        \n",
    "        batch_y = data_y[inx].copy()\n",
    "        batch_X = np.empty((batch_size,data_X[0].shape[1],unique[count]),dtype=np.float32)\n",
    "        for j,i in enumerate(inx):\n",
    "            batch_X[j] = data_X[i].T.copy()\n",
    "            \n",
    "        batch_y = (batch_y - energy_mean)/energy_var\n",
    "        yield batch_X,batch_y.reshape(-1,1)\n",
    "        \n",
    "def val_data_generator(data,batch_size):\n",
    "    data_X = data[0]\n",
    "    data_y = data[1]\n",
    "    lens = np.array([len(i) for i in data_X])\n",
    "    inxs = np.argsort(lens)\n",
    "    lens = lens[inxs]\n",
    "    lo = 0\n",
    "    while(lo < len(inxs)):\n",
    "        natoms = lens[lo]\n",
    "        hi = min(lo+batch_size,np.searchsorted(lens[lo:],natoms,'right')+lo)\n",
    "        batch_y = data_y[inxs[lo:hi]].copy()\n",
    "        batch_X = np.empty((hi-lo,data_X[0].shape[1],natoms),dtype=np.float32)\n",
    "        for j,i in enumerate(inxs[lo:hi]):\n",
    "            batch_X[j] = data_X[i].T.copy()\n",
    "        lo = hi\n",
    "        batch_y = (batch_y - energy_mean)/energy_var\n",
    "        yield batch_X,batch_y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_split(data,val_ratio,seed = 0):\n",
    "    inx = np.arange(len(data[0]),dtype=int)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(inx)\n",
    "    sl = int(val_ratio*len(data[0]))\n",
    "    train_X = []\n",
    "    val_X = []\n",
    "    for i in range(sl):\n",
    "        val_X.append(data[0][inx[i]])\n",
    "    for i in range(sl,len(data[0])):\n",
    "        train_X.append(data[0][inx[i]])\n",
    "    return (train_X,data[1][inx[sl:]]),(val_X,data[1][inx[:sl]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_features = bechler_f\n",
    "energy_mean = data_features[1].mean()\n",
    "energy_var = np.sqrt(((data_features[1]-energy_mean)**2).mean())\n",
    "input_shape = (None,data_features[0][0].shape[1],None)\n",
    "train_data,val_data = train_test_split(data_features,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 462, None)\n",
      "(None, 462)\n",
      "(None, 1024)\n",
      "(None, 1024, None)\n",
      "(None, 1024)\n"
     ]
    }
   ],
   "source": [
    "def build1(t_input,input_shape = input_shape):#batch,fetures,atoms\n",
    "    def make_dense(l,n,name,nonl = rectify):\n",
    "        res = BatchNormLayer(l,name = 'bn_'+name)\n",
    "        res = DenseLayer(res,n,name = 'fc_'+name,nonlinearity=nonl)\n",
    "        return res\n",
    "    \n",
    "    input_l = InputLayer(input_shape,input_var = t_input)\n",
    "    print input_l.output_shape\n",
    "    bcast_l = BroadcastLayer(input_l,[0,2])\n",
    "    print bcast_l.output_shape\n",
    "    dense = make_dense(bcast_l,512,'1')\n",
    "    dense = make_dense(dense,1024,'3',nonl=rectify)\n",
    "    print dense.output_shape\n",
    "    unbcast = UnbroadcastLayer(dense,bcast_l)\n",
    "    print unbcast.output_shape\n",
    "    glpool = GlobalPoolLayer(unbcast,T.sum)\n",
    "    print glpool.output_shape\n",
    "    dense = make_dense(glpool,512,'1')\n",
    "    dense = make_dense(dense,256,'3',nonl=rectify)\n",
    "\n",
    "    res = DenseLayer(dense,1,nonlinearity=identity)\n",
    "    return res\n",
    "\n",
    "def build(t_input,input_shape = input_shape):#batch,fetures,atoms\n",
    "    def make_dense(l,n,name,nonl = rectify):\n",
    "        res = BatchNormLayer(l,name = 'bn_'+name)\n",
    "        res = DenseLayer(res,n,name = 'fc_'+name,nonlinearity=nonl)\n",
    "        return res\n",
    "    \n",
    "    input_l = InputLayer(input_shape,input_var = t_input)\n",
    "    print input_l.output_shape\n",
    "    bcast_l = BroadcastLayer(input_l,[0,2])\n",
    "    print bcast_l.output_shape\n",
    "    dense = make_dense(bcast_l,512,'1')\n",
    "    dense = make_dense(dense,1024,'3',nonl=rectify)\n",
    "    print dense.output_shape\n",
    "    unbcast = UnbroadcastLayer(dense,bcast_l)\n",
    "    print unbcast.output_shape\n",
    "    glpool = GlobalPoolLayer(unbcast,T.sum)\n",
    "    print glpool.output_shape\n",
    "    dense = make_dense(glpool,512,'1')\n",
    "    dense = make_dense(dense,256,'3',nonl=rectify)\n",
    "    res = DenseLayer(dense,1,nonlinearity=identity)\n",
    "    return res\n",
    "\n",
    "\n",
    "fatures = T.tensor3('fetures')\n",
    "energies = T.matrix('energy')\n",
    "net = build(fatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "train_fn compiled\n",
      "test_fn compiled\n"
     ]
    }
   ],
   "source": [
    "pred = get_output(net)\n",
    "params = get_all_params(net, trainable=True)\n",
    "loss = lasagne.objectives.squared_error(pred,energies).mean()\n",
    "G_lr = theano.shared(np.array(0.001, dtype=theano.config.floatX))\n",
    "updates = lasagne.updates.adam(loss,params,G_lr)\n",
    "print ('start')\n",
    "train_fn = theano.function([fatures,energies],[loss], allow_input_downcast=True, updates=updates)\n",
    "print ('train_fn compiled')\n",
    "test_fn = theano.function([fatures,energies],[loss], allow_input_downcast=True)\n",
    "print ('test_fn compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_weights(network, name ):\n",
    "    np.savez(name+\".npz\", **{\"param%d\" % i: param for i, param in enumerate(get_all_param_values(network))})\n",
    "             \n",
    "def load_weights(network,name ):\n",
    "    f = np.load(name+\".npz\")\n",
    "    params = [f[\"param%d\" % i] for i in range(len(f.files))]\n",
    "    f.close()\n",
    "    set_all_param_values(network,params)\n",
    "\n",
    "def train(num_epoch,train_data_genertor,val_data_genertor,\n",
    "          train_fn=train_fn,\n",
    "          test_fn=test_fn,\n",
    "          net=net,\n",
    "          model_prefix='model',\n",
    "          G_lr = G_lr,\n",
    "          lr_sh = [100,150]):\n",
    "    \n",
    "    log = open('train.log','a')\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        train_num_batch = 0.\n",
    "        train_loss = 0.\n",
    "        for i,batch in enumerate(train_data_genertor()):\n",
    "            train_loss += train_fn(batch[0],batch[1])[0]\n",
    "            train_num_batch += 1.\n",
    "            if(i % 10 == 0):\n",
    "                print '\\r %i %f'%(i,train_loss/train_num_batch),\n",
    "                log.write('%i %f\\n'%(i,train_loss/train_num_batch))\n",
    "        val_loss = 0.\n",
    "        val_num_batch = 0.\n",
    "        for i,batch in enumerate(val_data_genertor()):\n",
    "            val_loss += test_fn(batch[0],batch[1])[0]\n",
    "            val_num_batch += 1.\n",
    "        save_weights(net,model_prefix+'repoch%itr%.3fval%.3f'%(epoch,train_loss/train_num_batch,val_loss/val_num_batch))\n",
    "        print ('\\repoch %i train_loss=%f val_loss=%f'%(epoch,train_loss/train_num_batch,val_loss/val_num_batch))\n",
    "        log.write('epoch %i train_loss=%f val_loss=%f\\n'%(epoch,train_loss/train_num_batch,val_loss/val_num_batch))\n",
    "        \n",
    "        if(epoch in lr_sh):\n",
    "            G_lr.set_value(G_lr.get_value()*np.float(0.01))\n",
    "        \n",
    "    log.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train_loss=1.085759 val_loss=1.210062\n",
      "epoch 1 train_loss=0.819076 val_loss=1.219164\n",
      "epoch 2 train_loss=1.053766 val_loss=1.195876\n",
      "epoch 3 train_loss=1.053764 val_loss=1.181652\n",
      "epoch 4 train_loss=1.052548 val_loss=1.162484\n",
      "epoch 5 train_loss=0.916806 val_loss=1.145805\n",
      "epoch 6 train_loss=0.728708 val_loss=1.143874\n",
      "epoch 7 train_loss=0.937789 val_loss=1.063739\n",
      "epoch 8 train_loss=0.694329 val_loss=1.020797\n",
      "epoch 9 train_loss=0.806675 val_loss=0.944785\n",
      "epoch 10 train_loss=0.786716 val_loss=0.804985\n",
      "epoch 11 train_loss=0.547948 val_loss=0.701317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-e9aaf14eb1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mval_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mval_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mG_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-aba879163277>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epoch, train_data_genertor, val_data_genertor, train_fn, test_fn, net, model_prefix, G_lr, lr_sh)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_genertor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mtrain_num_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nvvaulin/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_generator = lambda : train_data_generator(train_data,500,200)\n",
    "val_generator = lambda : val_data_generator(val_data,500)\n",
    "G_lr.set_value(np.float(0.001))\n",
    "train(250,train_generator,val_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
